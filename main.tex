\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage[most]{tcolorbox}
\usepackage[dvipsnames]{xcolor}

% if you ever want to ente% Required for inserting images

\title{Analysis Notes 131AH}
\author{OMKAR TASGAONKAR}
\date{Winter Quarter - January-March 2026}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{axm}{Axiom}[section]
\newtheorem{note}{Note}[section]

\tcolorboxenvironment{thm}{
  colback=lime,
  colframe=black,
  breakable
}

\tcolorboxenvironment{lem}{
  colback=yellow,
  colframe=black,
  breakable
}

\tcolorboxenvironment{defn}{
  colback=pink,
  colframe=black,
  breakable
}

\tcolorboxenvironment{cor}{
  colback=SpringGreen,
  colframe=black,
  breakable
}

\tcolorboxenvironment{axm}{
  colback=white,
  colframe=black,
  breakable
}

\tcolorboxenvironment{note}{
  colback=lightgray,
  colframe=black,
  breakable
}

\setlength{\parindent}{0pt}

\begin{document}

\maketitle
\section*{If you are reading this...}
Hello! These are my notes for analysis, which I have typed up to save to github so I can reference them in the future. Things will be written in my own words and may not be fully correct, but this is just my attempt to fully internalize everything and write it down as precisely as possible.
\tableofcontents
\pagebreak

\section{Propositional and First-Order Logic}
\begin{defn}
    A proposition is a statement that takes a TRUE or FALSE value
\end{defn}
For example, "birds are mammals" is a valid proposition, which takes on the value of FALSE. We also define a "primitive proposition", which is one with no connectives or quantifiers.\\

\begin{defn}
    A connective is a unary or binary operator allowing us to chain propositions to create new propositions
\end{defn}
The connectives we will work with are $\lnot$ (logical not), $\land$ (and), $\lor$ (or), $\implies$ (implies), and $\iff$ (bioconditional). The $\lnot$ operator is the only unary operator, which switches the value of the proposition. The others are binary, requiring two propositions and outputting one value.\\

We view an interesting truth table for $\implies$:
\begin{center}
\begin{tabular}{| c | c  | c |}
\hline
$P$ & $Q$ & $P \implies Q$ \\
\hline
T & T & T\\
T & F & F\\
F & T & T\\
F & F & T\\
\hline
\end{tabular}
\end{center}

\indent When $P$ is false but $Q$ is true, the entire statement is "vacuously" true as the expected proposition $Q$ is true no matter $P$. When both are false, the proposition is true as well. So in general when $P$ is false, $P \implies Q$ is true.\\

\begin{lem}
    $(P \implies Q) \iff \lnot(P \land \lnot Q) \iff \lnot P \lor Q$
\end{lem}
Here the biconditional means that the propositions are equivalent. We can use the biconditional to represent when two statements imply each other or are logically the same.\\

This lemma highlights proof by contradiction, where we assume $P$ and $\lnot Q$ and we show some contradiction (or show that the proposition is false). Then the logical not of the proposition is true. The second equivalence is an application of DeMorgan's Laws for logic.\\

\begin{lem}
    $(P \implies Q) \iff \lnot Q \implies \lnot P$
\end{lem}
This is the method of proof by contrapositive. This implication is called the \textbf{"only if"} direction, while the reverse is the \textbf{"if"} direction.\\

\begin{defn}
    We propose two quantifiers: $\forall$ (for all/for each) and $\exists$ (there exists)
\end{defn}
A first order statement assumes the form: (quantifier)(variable): (proposition). This is the basis of First Order Logic, an extension of propositional logic. For example:
\begin{equation*}
    \forall x : P(x)
\end{equation*}
translates to: for all x such that $P(x)$ is true. The terminology "some" is equivalent to $\exists$.\\

\begin{lem}
    $\lnot(\forall x : P(x)) \iff \exists x : \lnot P(x)$ and $\lnot(\exists x: P(x)) \iff \forall x : \lnot P(x)$
\end{lem}
Here we see how the logical not operator distributes over quantifier and proposition.\\

\begin{thm}
    $\forall x \forall y: P(x, y) \iff \forall y, \forall x: P(x, y)$ and $\exists x, \exists y: P(x, y) \iff \exists y, \exists x: P(x, y)$
\end{thm}
If the quantifiers for two variables are the same, we can switch the order of the quantifiers. This will appear in proofs involving sets and supremums/infimums. However, if they are not the same, then we cannot suggest an equivalence.\\

\begin{thm}
    $\exists x \forall y: P(x, y) \implies \forall y \exists x: P(x, y)$
\end{thm}
The reason why this is a one-way implication is because the left hand statement is more specific. We will encounter this when dealing with images and preimages. We take the following example:\\

\textbf{Mathematical Example}: If there exists one $x$ greater than all $y$, then for each $y$ there exists an $x > y$. In this case that $x$ is the same. For the converse, if each $y$ has an $x > y$, there may not be a singular $x$ greater than all $y$.\\

\textbf{Figurative Example}: If there is one building to which all apartments belong, then all the apartments belong to that building. However, if all the apartments belong to \text{a} building, not necessarily they belong to one building. We can have the apartments in multiple buildings.
\pagebreak

%------------------------------------------------------------%

\section{Naive Set Theory and Zermelo Fraenkel}
\subsection{Naive Set Theory}
Naive Set Theory was the initial proposed set theory. A set being a container for some sort of objects. To construct a set, we have the following:
\begin{defn}
    The Comprehension Principle: $A \coloneq \{x : P(x)\}$
\end{defn}
The symbol $\coloneq$ means "defined as". If we use "$=$" when referring to sets, that is not the same as using $\coloneq$.\\

The Comprehension Principle tells us that a set is some elements $x$ that satisfy a proposition $P(x)$. We won't dwell on this theory too much, but it has pitfalls such as Russell's paradox:
\begin{equation*}
    A \coloneq \{x : x \notin x\}
\end{equation*}

Here if $x \notin x$, then we could say $A \notin A$, but the proposition tells us that $A \in A$, so we come across a contradiction.\\

\subsection{Zermelo Fraenkel}
\begin{axm}
    \textbf{Axiom of Separation} Any set $A$ can be defined as $\{x \in U : P(x)\}$.
\end{axm}
We see that the elements $x \in A$ are being drawn from $U$ which in this case can be a universe (all the possible sets, but this is not itself a set), or a bigger set. By restricting elements, we can prevent self references such as those in Russell's paradox.\\

\begin{axm}
    \textbf{Axiom of Extensionality} $\forall A, B: A = B \iff \forall x: (x \in A \iff x \in B)$
\end{axm}
We mentioned previously that $\coloneq$ is not the same as $=$ for sets. Extensionality gives the definition of $=$ to be that the sets must have the same elements. The axiom itself is sometimes written as an implication rather than a biconditional.\\

\begin{axm}
    \textbf{Empty Set}: $\exists \varnothing : \forall x \in U: x \notin \varnothing$
\end{axm}
We postulate the existence of the empty set, which contains none of the possible elements in the universe. Now, how do we build the set of all subsets (aka the powerset)?\\

\begin{defn}
    $\subseteq$ (subset relation): $\forall A, B: A \subseteq B \implies \forall x \in A: x \in B$
\end{defn}
Let's try to use the Axiom of Separation and this definition to construct the powerset. We can say maybe for a set $A$ that $\mathcal{P}(A) \coloneq \{B \in C: B \subseteq A\}$. But this definition requires a set $C$ to exist in which $B$ belongs, so either this is the powerset or a set containing more elements than the powerset, and so we have a circular definition.\\

\begin{axm}
    \textbf{Powerset}: $\forall A \exists \mathcal{P}(A): (\forall B \subset A: B \in \mathcal(P)(A))$
\end{axm}
We have the same issue when we go to define a set such as $\{x, y\}$, which is called the pairset, as each element is also inherently a set.\\

\begin{axm}
    \textbf{Pairset}: $\forall x, y \exists A \forall Z \in A: x = z \lor y = z$
\end{axm}
The pairset allows us to show the existence of a singleton set $\{x\}$ if both the elements given are the same as $\{x, x\} = \{x\}$. This can be shown by extensionality.\\

\begin{axm}
    \textbf{Infinite Set}: $I \coloneq \{x: x \in I \implies \{x\} \in I\}$
\end{axm}
This is one possible construction of the infinite set we will use for the naturals; however, this axiom postulates the existence of infinite sets. Many of the constructions in analysis will hinge on this axiom; without it, we fail to construct the idea of "natural numbers" let alone "real numbers".\\
\pagebreak

%----------------------------------------------------------------------------%

\section{Ordered Pairs and Relations}
We talk about pairs (or n-tuples) of numbers such as $(1, 2)$ or $(x, y)$. However, we also distinguish the order unlike sets, saying that $(1, 2)$ is not the same as $(2, 1)$.\\

\begin{defn}
    \textbf{Kuratowski Pair}: $(x, y) \coloneq \{\{x\}, \{x, y\}\}$
\end{defn}
\pagebreak

%---------------------------------------------------------------------%
\section{Isomorphisms of Dedekind Cuts and Ordered Fields}
So far we have constructed the reals or $\mathbb{R}$ as the set of Dedekind cuts, which are the set of rationals less than that real number. By allowing for completeness, we can show that each cut admits an upper bound so it admits a supremum.\\

The supremum is the real number itself; however, quantifying the supremum will require the existence of an algebraic structure that encompasses the supremum. If we solely rely on $\mathbb{Q}$, then certain cuts may not admit supremums in $\mathbb{Q}$.\\

So we say there exists some complete ordered field $(F, +, 0, \cdot, 1, \leqslant)$ (completeness is important here, otherwise we cannot guarantee supremums). Each ordered field fundamentally has the $\mathbb{N}_F$. We examine a theorem:
\begin{thm}
    If $F$ is an ordered field, then it has characteristic $0$
\end{thm}
\noindent We prove this by contrapositive:
\begin{proof}
Assume $F$ does not have characteristic 0. Then $\sum_{k=1}^{n}(1) = 0$ for an arbitrary n. We then know that $0 < 1$, by ordered field properties:
\begin{equation*}
    0 < 1 \implies 0 < \sum_{k=1}^{n-1}(1) < \sum_{k=1}^{n-1}(1) + 1 = \sum_{k=1}^{n}(1)
\end{equation*}
Now assume the inequality to be true, then by mulitplicative property of ordered fields:
\begin{equation*}
    \implies \sum_{k=1}^{n-1}(1) \cdot \sum_{k=1}^{n-1}(1) < \sum_{k=1}^{n}(1) \cdot \sum_{k=1}^{n-1}(1)
\end{equation*}
\begin{eqnarray}
    \iff (n-1)^2 = n^2 - 2n + 1 < n(n-1) = n^2 - n
\end{eqnarray}
We know that $n$ or $1$ added $n$ times repeatedly is equal to zero, so we get:
\begin{equation*}
    0 + 1 = 1 < 0
\end{equation*}
Which is a contradiction of a key property of ordered fields that $0 \leqslant 1$. 
\end{proof}
\noindent So now, the addition of of $1$ to an element of the field becomes our sucessor operation, and $0$ becomes our zero element. If we then consider the subset of elements greater than zero, then zero is not a sucessor of any number in the set. Thus we can construct the naturals in a field as:
\begin{equation*}
    \mathbb{N}_F = \bigcap_{\alpha \in I}\{A_\alpha \subseteq F: (\forall x \in A_\alpha: x +1 \in A_\alpha)\}
\end{equation*}

\noindent Now that we have the naturals in every ordered field, by field properties, we require their additive and multiplicative inverses. Furthermore any combination of these must also remain in the field, giving rise to the rationals $\mathbb{Q}_F$ (considered the "minimal" ordered field).\\

\noindent So currently we have the set of Dedekind cuts $\mathbb{R}$ and an ordered field structure $F$. One is constructed from ground up, the other's existence is assumed respectively. We aim to show the two are isomorphic. We know the following theorem proved in prior chapters:
\begin{thm}
The rationals of different fields are isomorphic
\end{thm}
\noindent So there exists an isomorphism we call $\psi: \mathbb{Q} \xrightarrow{} \mathbb{Q}_F$. Since the Dedekind cuts are merely sets of rationals, then we can construct cuts in $F$ which we call $\mathbb{R}_F$. We can extend $\psi: \mathbb{R} \xrightarrow{} \mathbb{R}_F$.\\

\begin{thm}
    $\forall A \in \mathbb{R}_F: (\exists b \in F, \forall a \in A: a \leqslant b): \sup: A \xrightarrow{} F$ and $\sup$ is an isomorphism
\end{thm}
\noindent This lemma is essentially the restatement of the completeness of $F$. If we take any subset bounded above, then it admits a supremum. However, here we define the supremum as a map from a set to an element in the field. We prove it is an isomorphism for a cut specifically. We require the lemma:
\begin{lem}
    $\forall A \in \mathbb{R}_F: A = \{a \in \mathbb{Q}_F: a < \sup(A)\}$
\end{lem}
\noindent For now we only show the inclusion $A \subseteq \{a \in \mathbb{Q}_F: a < \sup(A)\}$.
\begin{proof}
If $x \in A$, then $x \leqslant \sup(A)$. If $x = \sup(A) \implies \sup(A) \in A$. By the definition of Dedekind cut's, there cannot be a maximal element, so:
\begin{equation*}
    \exists b \in A: \sup(A) < b \land b \leqslant \sup(A)
\end{equation*}
The second inequality comes from the supremum definition. We see that we get a contradiction, so $\sup(A) \notin A$, thus $\forall a \in A: a < \sup(A)$.
\end{proof}

\noindent We now return to proving the theorem at hand.
\begin{proof}
For the sake of brevity we assume $\sup$ is a homomorphism (which means that it respects the operations of sets and the field). We aim to show injectivity:
\begin{equation*}
    \forall A, B: \sup(A) = \sup(B) \implies A = B
\end{equation*}
If two cuts are not equal, then either $A \subset B \lor B \subset A$. Assume without loss of generality that $A \subset B$. Then:
\begin{equation*}
    \exists b \in B, \forall a \in A: a < b
\end{equation*}
This inequality we can derive from the ordering of $F$ and the construction of the cuts. Then $b$ is an upper bound for $A$ and by definition of supremum $\sup(A) \leqslant b$. So by the lemma $a < \sup(A) \leqslant b < \sup(B)$. Thus $\sup(A) < \sup(B)$, proving injectivity of $\sup$.
\end{proof}

\begin{proof}
We now prove the surjectivity of $\sup$:
\begin{equation*}
    \forall x \in F, \exists A \in \mathbb{R}_F: \sup(A) = x
\end{equation*}
Assume for a certain $x \in F$, that there does not exist an $A \in \mathbb{R}_F$ such that $\sup(A) = x$. Then all Dedekind cuts must admit a supremum in $F$ as they are bounded above. (The following part may be a bit hand wavy :), but in essence by means of pigeonhole principle (whose proof we detailed before), two different cuts will then share the same supremum, which breaks injectivity. 
\end{proof}

\medskip
\begin{thm}
For all complete ordered fields $(F, +, 0, \cdot, 1, \leqslant)$, we find that $\mathbb{R} \cong F$.
\end{thm}
The proof of this is simply a combination of the previous theorems and lemmas. We know that $\psi: \mathbb{R} \xrightarrow{} \mathbb{R}_F$ is an isomorphism and $\sup: \mathbb{R}_F \xrightarrow{} F$ is also an isomorphism. The composition is therefore an isomorphism:
\begin{equation*}
    \sup \circ \psi: \mathbb{R} \xrightarrow{} F
\end{equation*}
So the Dedekind cut representation of the reals are isomorphic to any complete ordered field. So now instead of dealing with the reals as a set of Dedekind cuts, we can deal with it as an ordered field of the supremums of the Dedekind cuts. So $\sqrt{2}$ is a supremum of the cut, so we can explicitly refer to the cut as $\sqrt{2}$ itself even though the cut does not admit a supremum in $\mathbb{Q}$. 
\pagebreak

%----------------------------------------------------------------------------------%

\section{Properties of the Reals}
We have determined that every complete ordered field is isomorphic to the real numbers. We now aim to show certain properties have correctly carried forward to this construction.\\

We dealt with rational roots and equalities such as $x^2 = 2$ that did not admit solutions in the rationals, but allowed us to construct $\sqrt{2}$ in the reals. We now see the reals hold for arbitrary roots as well:
\begin{thm}
$\forall a \in \mathbb{R}^+, \forall n \in \mathbb{N}\setminus \{0\}: (\exists x \in \mathbb{R}^+: x^n = a) \land (x^n = y^n \implies x = y)$
\end{thm}
In essence, there exists an $nth$ root for every real number, which is unique. We use the following property:
\begin{equation*}
    \forall x, y \in \mathbb{R}, \forall n \in \mathbb{N}\setminus \{0\}: x^n - y^n = (x-y)\sum_{k=0}^{n-1}x^k y^{n-k-1}
\end{equation*}
Integer exponentiation can be defined by field properties as repeated multiplication of the element in the field.\\

\textbf{HW 5 Problem}: We prove the property below:
\begin{proof}
By the distributive property of multiplication in fields:
\begin{equation*}
    (x-y)\sum_{k=0}^{n-1}x^k y^{n-k-1} = x\sum_{k=0}^{n-1}x^k y^{n-k-1} - y\sum_{k=0}^{n-1}x^k y^{n-k-1}
\end{equation*}
\begin{equation*}
    = \sum_{k=0}^{n-1}x^{k+1} y^{n-k-1} - \sum_{k=0}^{n-1}x^k y^{n-k}
\end{equation*}
We can notice a pattern of cancellations here. The last term of the left term is $x^n$ and the first term of the right term is $y^n$. We can then rewrite the sum as:
\begin{equation*}
    = x^n - y^n + \sum_{k=0}^{n-2}x^{k+1} y^{n-k-1} - \sum_{k=1}^{n-1}x^k y^{n-k}
\end{equation*}
We see that the two summations output the same terms, just that the starting and ending indices are up by 1. If our starting term is $x y^{n-1}$, then at $k=0$ we sum $x^{k+1}y^{n-k-1}$, but at $k=1$, we sum $x^k y^{n-k}$.\\

If our last term is $x^{n-1}y$, then the same applies. So the two summations are equal, thus their subtraction equals 0, and our resultant expression is $x^n - y^n$.
\end{proof}

\medskip
We return to the proof of the theorem. We define the set $A \coloneq \{y \in \mathbb{R}^+: y^n \leqslant a\}$. Since this is a subset of the field, if it is bounded, then it admits a supremum:
\begin{equation*}
    1 + a > a \implies (1 + a)^n > (1+a)^{n-1}a > a^n > a \ \ (\text{Ordering Property of Multiplication})
\end{equation*}
Thus $\forall y \in A: y \leqslant 1 + a$ so $A$ is bounded above, thus it admits a supremum. We want to show that $\sup(A)^n = a$, thus defining the supremum of the set as the n-th root and showing its existence (as we know supremum exists).
\begin{proof}
We use the fact that $\forall y \in A: y \leqslant \sup(A)$, We first show $\sup(A)^n \leqslant A$. We also use the fact that:
\begin{equation*}
    \forall m \in \mathbb{N}\setminus \{0\}: \sup(A) - \frac{1}{m} < \sup(A) \implies \exists y \in A: \sup(A) - \frac{1}{m} \leqslant y
\end{equation*}
\begin{equation*}
    \sup(A)^n - a \leqslant \sup(A)^n - y^n = (\sup(A) - y)\sum_{k=0}^{n-1}\sup(A)^k y^{n-k-1}
\end{equation*}
Our previous implication that $\sup(A) - \frac{1}{m} \leqslant y \iff \sup(A) \leqslant y + \frac{1}{m} \implies \sup(A) \leqslant y$. Since $y \in A$, we know that $y \leqslant \sup(A)$, so $y = \sup(A)$. Our inequality becomes:
\begin{equation*}
    \sup(A)^n - a \leqslant \sup(A)^n - y^n = (\sup(A) - y)\sum_{k=0}^{n-1}\sup(A)^k y^{n-k-1} = 0 \iff \sup(A)^n \leqslant a
\end{equation*}

\medskip
We now prove that $\sup(A)^n \geqslant a$. We note that $\forall m \in \mathbb{N}\setminus \{0\}: \sup(A) + \frac{1}{m} > \sup(A)$, so:
\begin{equation*}
    (\sup(A) + \frac{1}{m})^n > a \implies a - \sup(A)^n < (\sup(A) + \frac{1}{m})^n - \sup(A)^n 
\end{equation*}
\begin{equation*}
    = \frac{1}{m}\sum_{k=0}^{n-1}(\sup(A)+\frac{1}{m})^k \sup(A)^{n-k-1} \leqslant \frac{n}{m}(\sup(A) + \frac{1}{m})^{n-1}
\end{equation*}
Thus, by dividing both sides (also the Archimedian Property is really a double implication), we see that:
\begin{equation*}
    \frac{m}{n}\frac{a - \sup(A)^n}{(\sup(A) + \frac{1}{m})^{n-1}} < 1 \implies a - \sup(A)^n \leqslant 0 \iff \sup(A)^n \geqslant a
\end{equation*}
So we have proven that $\sup(A)^n = a$, proving the existence of the $nth$ root, which we will denote by $a^{1/n}$.
\end{proof}

\medskip
\begin{proof}
We now prove the uniqueness of the $n-th$ root. Assume there are two $n-th$ roots of $a$ such that $y^n = a = \tilde{y}^n$. Then:
\begin{equation*}
    \sup(A)^n = y^n = \tilde{y}^n = a \implies a^{1/n} = \sup(A) = y = \tilde{y}
\end{equation*}
Since the supremum is unique, then if $y = \sup(A) = \tilde{y}$, then $y$ must equal $\tilde{y}$, so the $nth$ root is unique.
\end{proof}

\centerline{\rule{\textwidth}{0.4pt}}
\bigskip
Now that we have the notion of natural powers and $nth$ roots, we can extend this to rational roots. We know that any rational can be represented as the ratio of two integers $p/q$. Note the following:
\begin{cor}
$\forall n \in \mathbb{N}, \forall a \in \mathbb{R}: a^{-n} = (a^{-1})^n = (\frac{1}{a})^n \land a^0 = 1$
\end{cor}
With this corollary, we are able to define the notion of integer powers (as integers have additive inverses not accounted for by natural powers). Another property of natural powers and by extension integer powers:
\begin{cor}
$\forall m, n \in \mathbb{N}: a^m \cdot a^n = a^{m+n} \land a^{m \cdot n} = (a^m)^n$
\end{cor}

\medskip
\textbf{HW 5 Problem}: Using this corollary, we can prove the following lemma. For this problem assume $a > 1$:
\begin{lem}
$\forall m, n, p, q \in \mathbb{Z}: n, q > 0 \land \frac{m}{n} = \frac{p}{q} \implies \forall a > 1: (a^m)^{1/n} = (a^p)^{1/q}$
\end{lem}
\begin{proof}
If $\frac{m}{n} = \frac{p}{q}$, then we know that $mq = np$ where the two are integers. If $m/n < 0$, then $p/n < 0$ and same for $m/n \geqslant 0$. Thus the two fractions take on the same sign. Therefore $mq = np$ will either be positive or negative.\\

If $mq = np < 0$, we simply define their additive inverses $m'q' = n'p' > 0$ and use them to construct an equality (eg $-a = -b \iff a = b$ by field properties). So without loss of generality we proceed with $mq = np > 0$:
\begin{eqnarray}
    a^{mq} = a^{qm} = (a^q)^m \land a^{np} = a^{pn} = (a^p)^n
\end{eqnarray}
This is true by properties of exponentiation with respect to naturals and field multiplication. Thus, by the existence of arbitary roots:
\begin{equation*}
    (a^q)^m = (a^p)^n \implies ((a^q)^m)^{1/n} = a^p \implies ((a^m)^q)^{1/n} = a^p
\end{equation*}
We need to show that we can exchange the places of $q$ and $1/n$, so we prove $(a^m)^{1/n} = (a^{1/n})^m: n > 0$:
\begin{equation*}
    ((a^m)^{1/n})^n = a^m \land ((a^{1/n})^m)^n = (a^{1/n})^{mn} = (a^{1/n})^{nm} = ((a^{1/n})^n)^m = a^m
\end{equation*}
Since both expressions to raised to the $n: n > 0$ are equal, then the $nth$ roots are also equal so $(a^m)^{1/n} = (a^{1/n})^m$.\\

So returning to our initial statement:
\begin{equation*}
    ((a^m)^q)^{1/n}  = a^p \implies ((a^m)^{1/n})^{q} = a^p \implies (a^m)^{1/n} = (a^p)^{1/q}
\end{equation*}
\end{proof}
\begin{cor}
$\forall p, q \in \mathbb{Z}, q \neq 0: a^\frac{p}{q} = (a^p)^{1/q} = (a^{1/q})^p$
\end{cor}

\medskip
\begin{lem}
$\forall r, s \in \mathbb{Q}: a^{r+s} = a^r \cdot a^s$
\end{lem}
\begin{proof}
We let $r = \frac{m}{n}$ and $s = \frac{p}{q}$, where $m, n, p, q \in \mathbb{Z} \land p, q \neq 0$, then $r + s = \frac{m}{n} + \frac{p}{q} = \frac{mq + np}{pq}$. Thus by corollary 5.3:
\begin{equation*}
    a^{r+s} = a^{(mq + np)/pq} = (a^{1/pq})^{mq + np}
\end{equation*}
Now $mq + np$ is an integer expression, by corollary 5.2, we can rewrite the expression as:
\begin{equation*}
    = (a^{1/pq})^{mq} \cdot (a^{1/pq})^np = (a^{mq/pq}) \cdot (a^{np/pq}) = a^{m/p} \cdot a^{n/q} = a^r \cdot a^s
\end{equation*}
\end{proof}
The idea of multiplicative inverses can now be brought about for rational powers. Let $p, q, m, n \in \mathbb{Z}: q, n \neq 0$, then:
\begin{equation*}
    a^{p/q} \cdot a^{m/n} = (a^{1/pq})^{mq + np} = 1
\end{equation*}
Using corollary 5.1, we account for the sign of $mq + np$. If the expression is greater than zero, than it is a natural number. If it is less than zero, we define its additive inverse $(mq + np)' > 0$ by field properties and change the base from $a$ to $a^{-1}$ as per the corollary.\\

Without loss of generality we proceed with the first case. Once again by corollary 5.1 stating that $a^0 = 1$:
\begin{equation*}
    mq + np = 0 \implies np = -mq
\end{equation*}
So by Lemma 5.2, $(a^{1/pq})^{mq + np} = (a^{1/pq})^{mq} \cdot (a^{1/pq})^{np} = (a^{1/pq})^{mq} \cdot (a^{1/pq})^{-mq}$. Since $mq \in \mathbb{Z}$, we again apply corollary 5.1 and 5.3:
\begin{equation*}
    (a^{1/pq})^{mq} \cdot (a^{1/pq})^{-mq} = a^{m/p} \cdot a^{-m/p} = 1
\end{equation*}
So we see that $(a^{m/p})^{-1} = a^{-m/p} = (a^{-1})^{m/p}$.

\medskip
Now that we have defined and proven the properties of rational powers, we need to define real powers, which we do via supremums again leveraging completeness of the underlying ordered field:
\begin{defn}
$a^x \coloneq \begin{cases}
    \sup\{a^z: z \in \mathbb{Q} \land z \leqslant x\}, \ \ a > 1\\
    \inf\{a^z: z \in \mathbb{Q} \land z \leqslant x\}, \ \ a < 1\\
    1, \ \ a = 1\\
\end{cases}$
\end{defn}
\begin{proof}
We prove the first case, where if $a > 1$ and $x \in \mathbb{Q}$, then $a^x = \sup\{a^z: z \in \mathbb{Q} \land z \leqslant x\}$. We know that $x < x+1$. By density of the rationals:
\begin{equation*}
  \exists r \in \mathbb{Q}: x < r < x+1  
\end{equation*}
Since $z \leqslant q$, then $z < r$. We now need to prove the lemma:
\begin{lem}
$\forall z, r \in \mathbb{Q}: z < r \implies a^z < a^r$. 
\end{lem}
If $z < r$, and $a > 1$, we see that $a^{r-z} > a > 1$ so by Lemma 5.2:
\begin{equation*}
    a^{r-z} = a^r \cdot a^{-z} = a^r \cdot (a^z)^{-1} > 1
\end{equation*}
Since $a > 1 > 0$, then $a^z > a > 0$. So if $(a^z)^{-1} < 0$. Thus:
\begin{equation*}
    a^r \cdot (a^z)^{-1} > 1 \implies 1 \cdot a^z < a^r \cdot (a^z)^{-1} \cdot a^z \iff a^z < a^r
\end{equation*}

Therefore $a^r$ is an upper bound, allowing for the set to admit a supremum. We now want to show that $a^x$ is the least upper bound. Consider any upper bound $a^p: p \in \mathbb{Q}$, then:
\begin{equation*}
    \forall z \leqslant x: a^z \leqslant a^p \implies a^x \leqslant a^p
\end{equation*}
If $a^p < a^x$, then $a^{x-p} > 1$. Since $a > 1$, then $a^{x-p} > a > 1$ only if $x > p$.
\begin{equation*}
    p < x \land z \leqslant x \implies \exists z \leqslant x: p < z \implies a^p < a^z
\end{equation*}
Thus, $a^p$ is not an upper bound for all $a^z$, so we have proven the statement by contrapositive.

\medskip
\begin{lem}
  $\forall x, y \in \mathbb{R}: a^{x+y} = a^x \cdot a^y$  
\end{lem}
We define $a^{x+y} = \sup\{a^z: z \in \mathbb{Q} \land z \leqslant x + y\}$.\\

Since $a^y = \sup\{a^p: p \in \mathbb{Q} \land p \leqslant y\}$ and $a^x = \sup\{a^q: q \in \mathbb{Q} \land q \leqslant x\}$:
\begin{equation*}
    a^x \cdot a^y = \sup\{a^q\} \cdot \sup\{a^p\} = \sup\{a^q \cdot a^p\} = \sup\{a^{q+p}\}
\end{equation*}
This follows from Lemma 5.2 and $\sup$ being a homomorphism. Multiplying the supremums combines the predicates in a logical $\land$ and results in a Minkowski product $\{a^{q+p}: q \leqslant x \land p \leqslant y\}$. By ordered field properties:
\begin{equation*}
    q \leqslant x \land p \leqslant y \implies q + p \leqslant x + y
\end{equation*}
By definition of the set of $a^{x+y}$, we see $\forall q \leqslant x, \forall p \leqslant y: a^{p+q} \in \{a^z: z \leqslant x + y\}$. Therefore:
\begin{equation*}
    \{a^{q+p}\} \subseteq \{a^z\} \implies \sup\{a^{q+p}\} \leqslant \sup\{a^z\} \iff a^x \cdot a^y \leqslant a^{x+y}
\end{equation*}
Thus $a^x \cdot a^y = \sup\{a^{q+p}: p \leqslant y \land q \leqslant x\} \leqslant \sup\{a^{z}: z \leqslant x + y\}  = a^{x+y}$. \\

We now prove the opposite inequality, that $a^{x+y} \leqslant a^x \cdot a^y$. We choose a fixed $\tilde{q} \leqslant x$ and a fixed $\tilde{p} \leqslant y$, then:
\begin{equation*}
    a^{x+y} = \sup\{a^{z}: z \leqslant x + y\}
\end{equation*}
For every z, we can pick $\tilde{p}, \tilde{q}$ such that by Lemma 5.3:
\begin{equation*}
    (\forall z \in x+y: \exists \tilde{p} \leqslant y, \tilde{q} \leqslant x): z \leqslant \tilde{p} + \tilde{q} \implies a^z \leqslant a^{\tilde{p} + \tilde{q}} = a^{\tilde{p}} \cdot a^{\tilde{q}}
\end{equation*}
Which means that since the choices of $\tilde{p}, \tilde{q}$ were arbitrary:
\begin{equation*}
    \forall z \leqslant x + y: a^z \leqslant a^{\tilde{p}} \cdot a^{\tilde{q}} \leqslant a^x \cdot a^y
\end{equation*}
By definition of supremum, $a^{x+y}$ is the least upper bound for all $a^z$ and we have shown that $a^x \cdot a^y$ is an upper bound, so:
\begin{equation*}
    a^z \leqslant a^x \cdot a^y \implies a^{x+y} \leqslant a^x \cdot a^y
\end{equation*}
Thus proving $a^{x+y} = a^x \cdot a^y$. This forces a multiplicative inverse to exist such that $a^x \cdot a^y = a ^ 0 = 1 \implies x + y = 0 \implies y = -x$. Thus $(a^x)^{-1} = a^{-x}$.
\end{proof}

\centerline{\rule{\textwidth}{0.4pt}}
\bigskip
Now that we have defined exponentiation with respect to a real valued base, along with elementary operations, we aim to define the inverse of exponentiation, which is the logarithm.
\begin{thm}
The exponential function: $f: \mathbb{R} \xrightarrow{} \mathbb{R}^+ \setminus \{0\}$ is a bijection
\end{thm}
We define $f(x) = b^x$ for any $a \in \mathbb{R}^+ \setminus \{1\}$. The proof is the same if $b > 1$ or $b < 1$. Then:
\begin{proof}
$f$ is injective if $\forall x, y \in \mathbb{R}^+: b^x = b^y \implies x = y$. We can use Lemma 5.4 and multiplicative inverses:
\begin{equation*}
    b^x = b^y \implies b^x \cdot (b^y)^{-1} = 1 \implies b^x \cdot b^{-y} = b^{x-y} = 1
\end{equation*}
Since $b > 1$, then by the definition of exponentiation, $x-y = 0$, so $x = y$.\\

If $f$ is surjective, then $\forall y \in \mathbb{R}^+. \exists x \in \mathbb{R}: b^x = y$. Here $x$ is called the logarithm of $y$ base $b$.\\

We fix $a > 1$, then by the results of problem 1:
\begin{equation*}
    a^n - 1 = (a-1)\sum_{k=0}^{n-1}a^k \cdot 1^{n-k-1} = (a-1)\sum_{k=0}^{n-1}a^k \geqslant \sum_{k=0}^{n-1}1^k = n(a-1) 
\end{equation*}
Now if we let $a$ be the nth root of $b$ such that $a = b^{1/n} > 1$, then:
\begin{equation*}
    a^n - 1 \geqslant n(a-1) \implies b - 1 \geqslant n(b^{1/n} - 1)
\end{equation*}
If $t > 1$ and $n > (b-1)/(t-1)$, then $b^{1/n} < t$:
\begin{equation*}
    n(b^{1/n}-1) \leqslant b - 1 < n(t-1) \implies b^{1/n} - 1 < t-1 \implies b^{1/n} < t
\end{equation*}
If $w$ is such that $b^w < y$, then $b^{w + (1/n)} < y$ for sufficiently large $n$; to see this, apply part (c) to $t = y \cdot b^{-w}$.
\begin{equation*}
    b^{1/n} < t \implies b^{1/n} < y \cdot b^{-w} \implies b^{(1/n) + w} < y
\end{equation*}
If $b^w > y$, then $b^{w - (1/n)} > y$ for sufficiently large $n$. Let $t = y^{-1} \cdot b^w > 1$:
\begin{equation*}
    b^{1/n} < t \implies b^{1/n} < y^{-1} \cdot b^w \implies y < b^{w - (1/n)}
\end{equation*}
Let $A$ be the set of all $w$ such that $b^w < y$, and show that $x = \sup(A)$ satisfies $b^x = y$.\\

If $b^x > y$, then $b^{x - (1/n)} > y$, which means that $x - \frac{1}{n}$ is a smaller lower bound, so $b^x$ is not greater than $y$. If $b^x < y$, then $b^{x + (1/n)} < y$, so $x + \frac{1}{n} \in A$, but $x < x + \frac{1}{n} \notin A$, showing a contradiction.\\

Thus $b^x = y$ and $x = \sup(A)$, proving the existence of a logarithm of $y$ base $b$ for any $y \in \mathbb{R}^+$, showing surjection. The uniqueness of $x$ can be derived from the injection or also through supremums:
\begin{equation*}
    b^x = b^{x'} = y \implies x = \sup(A) \land x' = \sup(A)
\end{equation*}
Since the supremum is unique, then $x = x'$, showing injectivity, thus $f$ is a bijection and therefore has an inverse which we will call $\log$.
\end{proof}
\begin{cor}
$\log_b: \mathbb{R}^+ \setminus \{0\} \xrightarrow{} \mathbb{R}$ such that $log_b(y) = x$
\end{cor}
Then the composition of $\log_b(b^x) = x$ and $b^{\log_b(x)} = x$. Here we see the $\log_b$ acting as a left and right inverse, which is the expected behavior of the inverse of a bijective function. (Remember a surjection only has a right inverse, and an injection only has a left inverse).
\pagebreak

%-------------------------------------------------------------------------%
\section{Countability and Cardinality}
The natural numbers or $\mathbb{N}$ are the backbone of the idea of "counting". When we say $0, 1, 2, 3...$ so on and so forth, we are iterating through the naturals. We define the following:
\begin{defn}
    The first $n$ naturals are $[0, n) \coloneq \{k \in \mathbb{N}: k < n\}$
\end{defn}
We can now introduce the notion of a finite set.
\begin{defn}
    A finite set $A$ is a set such that $\exists f, \exists n \in \mathbb{N}: f: [0, n) \xrightarrow{} A$ and $f$ is a bijection
\end{defn}
An infinite set is one that is not finite, or one for which we cannot find a bijection between an interval of the first $n$ naturals for some $n \in \mathbb{N}$.\\

We also introduce the notion of Dedekind infinite. A set that is Dedekind infinite is one where we can find an injective but not surjective map between the set and itself. For example:
\begin{equation*}
    S: \mathbb{N} \xrightarrow{} \mathbb{N}: 0 \notin \text{Ran}(S) \implies S(\mathbb{N}) \subseteq \mathbb{N}
\end{equation*}
So the naturals are a Dedekind infinite set under the successor operation. We can also come up with other injections such as $f(n) = n^2$. We say the following:
\begin{lem}
Given a set $A$: A is Dedekind infinite $\implies$ A is infinite
\end{lem}
The converse requires invoking axiom of choice as we need to assume the existence of a choice function that can generate a sequence of values to which we can apply an injective function.\\

If we can find a bijection $f$ between an interval $[0, n)$ and a finite set $A$, then the value $n \in \mathbb{N}$ is called the cardinality of $A$, denoted by $|A|$. We see some inequalities related to cardinality:
\begin{lem}
For finite sets $A$ and $b$:
\begin{enumerate}
    \item $B \subseteq A \implies |B| \leqslant |A|$
    \item $|A \cup B| \leqslant |A| + |B|$
    \item $|A \times B| = |A| \cdot |B|$
\end{enumerate}
\end{lem}
We prove Lemma 6.2.1:
\begin{proof}
We define $|A|$ as $n \in \mathbb{N}, \exists f: f: [0, n) \xrightarrow{} A \land \text{Im}(f) = A$. The proof is as follows:
\begin{enumerate}
    \item $B \subseteq A \iff \forall x \in B: x \in A$ (Subset Definition)
    \item $\forall x \in B: (x \in A \implies x \in \text{Im}(f))$ (By definition of $f$)
    \item $\implies \forall x \in B \land A, \exists m \in [0, n): f(m) = x$ (Definition of Preimage)
    \item Thus $f^{-1}(B) = \{m \in [0, n): f(m) = x\}$
    \item Since $\forall m \in f^{-1}(B): m \in [0, n) \implies f^{-1}(B) \subseteq [0, n)$ (Definition of Subset)
    \item Therefore either $f^{-1}(B) = [0, n) = \text{Dom}(f)$ or $f^{-1}(B) \subset [0, n)$ so it does not contain all first $n$ naturals
    \item Since $f$ is a bijection, then $f|_{f^{-1}(B)}: f^{-1}(B) \xrightarrow{} B$ is a bijection, so by line 6: $|B| \leqslant |A|$
\end{enumerate}
\end{proof}
We prove Lemma 6.2.2:
\begin{proof} The proof is as follows. Since $A, B$ are finite, axiom of choice is not required:
\begin{enumerate}
    \item $A \cup B \iff \forall x \in A \cup B: x \in A \lor x \in B$ (Definition of Union)
    \item $f: [0, n) \xrightarrow{} A \land g: [0, m) \xrightarrow{} B$ such that $f, g$ are bijections. Then $|A| = n, |B| = m$.
    \item We define a function $p: [0, m+n) \xrightarrow{} A \cup B$ such that $\text{Ran}(p|_{[0, n)}) = A \land \text{Ran}(p|_{[n, m+n)}) = B$
    \item If $A \cap B \neq \varnothing$, then $\exists x: x \in A \land x \in B$
    \item $\implies x \in \text{Im}(f) \land \text{Im}(g)$
    \item $\implies \exists l \in [0, n) \land \exists k \in [m, m+n): p|_{[0, n)}(l) = p|_{[n, m+n)} = x$, which means that $p$ is not injective, so $|A \cup B| < m + n = |A| + |B|$
    \item If $A \cap B = \varnothing$, then $p$ is a bijection, so $|A \cup B| = m + n = |A| + |B|$
    \item Thus $|A \cup B| \leqslant |A| + |B|$
\end{enumerate}
\end{proof}
We prove Lemma 6.2.3:
\begin{proof} 
The proof is as follows:
\begin{enumerate}
    \item We define the cartesian product of $A, B$ as $A \times B \coloneq \bigcup \{f: [0, 2) \xrightarrow{} A \cup B: (f(0) \in A \land f(1) \in B)\}$
    \begin{enumerate}
        \item Remember that a function is fundamentally its graph, so each function is bijective to the set of pairs with the first element being from $A$ and the seocnd being from $B$.
        \item Let's restrict each $f$ to a single, unique ordered pair so that all the graphs are disjoint.
    \end{enumerate}
    \item Thus $|A \times B| = |\bigcup \{f: [0, 2) \xrightarrow{} A \cup B: (f(0) \in A \land f(1) \in B)\}| = \# \ \text{of} \ f$
    \item Let $|A| = m \land |B| = n$
    \item Fix an $a \in A: f(0) = a \land f(1) \in B \implies |\{f: [0, 2) \xrightarrow{} A \cup B: f(0) = a \land f(1) \in B\}| = n$ as there are $n$ elements in $B$
    \item Since there are $m$ elements in $A$, we have $m$ sets  $\{f: [0, 2) \xrightarrow{} A \cup B: f(0) = a \land f(1) \in B\}$.
    \item Thus in total we have $m$ sets of $n$ functions, so we have $m \cdot n$ functions in total, thus:
    \item $|A \times B| = |A| \cdot |B|$
\end{enumerate}
\end{proof}
We note the relation of Axiom of Choice and Cartesian Products/Powers. Take for example the cartesian power $\mathbb{R}^\mathbb{N}$ which is the set of real valued sequences. The question is the same as the problem proposed in Lemma 6.1. To construct the sequence, we encounter the following collection:
\begin{enumerate}
    \item $x_0 \in \mathbb{R}$
    \item $x_1 \in \mathbb{R}\setminus \{x_0\}$
    \item $x_2 \in \mathbb{R}\setminus \{x_0, x_1\}$
    \item $x_3 \in \mathbb{R}\setminus \{x_0, x_1, x_2\}$
    \item The sequences continues countably infinitely (for as many naturals there are)
\end{enumerate}
Here we see that we are required to select real numbers from an infinite family of infinite sets. We encounter the same problem for an infinite family of finite sets.\\

We now consider the cartesian power $\mathbb{N}^\mathbb{N}$ which is the set of all natural-valued sequences. We see the following family:
\begin{enumerate}
    \item $x_0 \in \mathbb{N}$
    \item $x_1 \in \mathbb{N}\setminus \{x_0\}$
    \item The sequence continues for all of $\mathbb{N}$
\end{enumerate}
This case does not require axiom of choice however, due to the well ordering principle. We know that every set of naturals admits a minimum (an infinimum that exists in the set). So we know that there exists one such possible sequence which is the naturals itself. We can also take different collections of subsets of $\mathbb{N}$, combined with the well-ordering, allowing us to define different sequences and their rules.\\

\textbf{Question to Think About}: What about $\mathbb{Q}^\mathbb{N}$, does such construction require Axiom of Choice?\\

\medskip
\begin{defn}
A countable set $A$ is one such that $\exists f$ such that $f: \mathbb{N} \xrightarrow{} A$ is a bijection
\end{defn}
We now exmaine countable sets. We will show results such as $\mathbb{Q}$ being countable, or the set of algebraic reals being countable, which on surface seem counterintuitive.\\

\medskip
\begin{thm}
If $A$ is countable, then $\forall B \subseteq A: B \ \text{infinite} \implies B \ \text{countable}$
\end{thm}
\begin{proof}
We first see that $A$ is infinite as well as it has an infinite subset (How can we prove this?)\\

Since $A$ is countable, then we can find some bijection $f: \mathbb{N} \xrightarrow{} A$. We then define $f^{-1}(B) = \tilde{B} \subseteq \mathbb{N}$. If $B$ is to be countable, then $\tilde{B}$ should also be countable. We prove by induction:\\

Let $B_0$ be our base case such that $B_0 = \tilde{B}$. We know that if $B$ is infinite then $\tilde{B}$ must be infinite (why?). We then define the following:
\begin{equation*}
    B_{k+1} = \begin{cases}
        \varnothing: \ \ B_k = \varnothing\\
        B_k \setminus \{\inf(B_k)\}: \ \ B_k \neq \varnothing
    \end{cases}
\end{equation*}
Since we are dealing with sets of naturals, we do not require Axiom of Choice for this infinite family. By the well ordering principle, we can pick out the infimum of each set.\\

If $B_{k+1}$ is infinite, then we see that $B_k = B_{k+1} \cup \{\inf(B_k)\}$, so $B_k$ is also infinite. What we have done is created the ocuntable sequence of sets:
\begin{equation*}
    \{B_k\}_{k \in \mathbb{N}} \coloneq (B_0, B_1, B_2, ..., B_k, B_{k+1},...) \ \text{such that} \ B_k = B_{k+1} \cup \inf(B_k)
\end{equation*}
If we manipulate the defining proposition of the sequence, we see that $B_k \setminus B_{k+1} = \inf(B_k)$.\\

So if we define some function $h: \mathbb{N} \xrightarrow{} \{B_k\}$, we get a bijection as there is a countable number of sets. We define the $\inf$ function:
\begin{equation*}
    \inf: \{B_k\}  \xrightarrow{} \tilde{B}
\end{equation*}
We first see that $\inf$ is injective. So $\forall B_x, B_y \in \{B_n\}: \inf(B_x) = \inf(B_y) \implies B_x = B_y$. Imagine $B_x \neq B_y$, then $B_x \subset B_y \lor B_y \subset B_x$. Assume without loss of generality that $B_x \subset B_y$:
\begin{equation*}
    \exists y \in B_y: y \notin B_x
\end{equation*}
Since each $B_k$ is infinite (implying the sets are not bounded above) and $B_x \subset B_y$, then $\forall x \in B_x: y < x$ by the ordering of the naturals. Thus $y$ is a lower bound for $B_x$ but $\inf(B_x) \in B_x$, so $y \neq \inf(B_x)$. Then:
\begin{equation}
    y \in B_y \land y \neq \inf(B_x) \implies \inf(B_y) \leqslant y < \inf(B_x)
\end{equation}
The inequality $y < \inf(B_x)$ follows from the fact that $y \neq \inf(B_x)$ and if $y > \inf(B_x)$, then $y \in B_x$ which it is not by definition. Thus $\inf(B_y) < \inf(B_x)$ proving the statement by contrapositive.\\

We now see that $\inf$ is surjective. $\forall b \in \tilde{B}, \exists B_k: \inf(B_k) = b$. This follows from the well ordering property, that every subset of the naturals has an infimum. So if we pick any element of $B$ which is a subset of the naturals, then through the recursive process, we can find a set:
\begin{equation*}
    \exists n \in \mathbb{N}: B_n \coloneq \{x \in \tilde{B}: b \leqslant x\}
\end{equation*}
The infimum of this set is $b$ and $B_n$ is a valid element in $\{B_k\}$ as it is either $\tilde{B} = B_0$ itself, or is a proper subset, which in that case $\exists B_{m}: B_{m} \subseteq B_0 \land B_n = B_m\setminus \inf(B_m)$.\\

Now that we know $\inf$ is also a bijection, we can construct a bijection $\inf \circ h: \mathbb{N} \xrightarrow{} \tilde{B}$ which means that $\tilde{B}$ is countable. Now:
\begin{equation*}
    \exists g: g: \tilde{B} \xrightarrow{} B \ \text{such that g is a bijection}
\end{equation*}
Thus $g \circ \inf \circ h: \mathbb{N} \xrightarrow{} B$, and the composition of two bijections is a bijection, showing that $B$ is countable.
\end{proof}

\medskip
\begin{cor}
For any set $A$: A finite or countable $\iff$ $\exists f: A \xrightarrow{} \mathbb{N}$ such that $f$ is an injection
\end{cor}
\begin{proof}
We examine the forward direction first. If $A$ is finite, then by definition 6.2 it is bijective to the subset of the first $n$ naturals, thus $f$ is an injection. If it is countable and not finite, then by definition 6.3, $f$ is a bijection so it is an injection.\\

For the converse, if $f$ is an injection, that means $\text{Ran}(f) \subseteq \mathbb{N}$. If $\text{Ran}(f) = \mathbb{N}$, then $f$ is also a surjection, so $A$ is infinite and countable.\\

If $\text{Ran}(f) \subset \mathbb{N}$, then if the range is infinite by Thm 6.1, it is countable, so there exists a bijection between $A$ and $\mathbb{N}$, so $A$ is countable.\\

Finally, if $\text{Ran}(f)$ is finite, then there exists a bijection between $A$ and the first $n$ naturals, so $A$ is finite.
\end{proof}

\medskip
\begin{lem}
If $A$ and $B$ are countable, then $A \times B$ is countable
\end{lem}
\begin{proof}
We define bijections $f: \mathbb{N} \xrightarrow{} A \land g: \mathbb{N} \xrightarrow{} B$. We can define a function $h: A \times B \xrightarrow{} \mathbb{N} \times \mathbb{N}$ such that $h(a, b) = (f^{-1}(a), g^{-1}(b))$.\\

We see that $h$ is a surjection:  This is supported by the bijections $f, g$.
\begin{equation*}
    \forall (m, n) \in \mathbb{N} \times \mathbb{N}, \exists (f(m), g(n)) \in A \times B: h((f(m), g(n))) = (m, n)
\end{equation*}
We now prove that $h$ is an injection: $\forall (a, b), (a', b') \in A \times B: h(a, b) = h(a', b') \implies (a, b) = (a', b')$:
\begin{equation*}
    h(a, b) = h(a', b') \implies f^{-1}(a) = f^{-1}(a') \land g^{-1}(b) = g^{-1}(b')
\end{equation*}
Since $f, g$ are bijections, their inverses are bijections, so they are also injections, thus $a = a' \land b = b'$.\\

We now define a map $\psi: \mathbb{N} \times \mathbb{N} \xrightarrow{} \mathbb{N}$ such that $\psi(m, n) = \frac{1}{2}(m+n)(m+n+1) + m$. This is called the Cantor Pairing Function. We see that every natural number can be represented by this (why?).\\

We now prove it is an injection. Given two ordered pairs $(m, n), (m', n')$, if $m < m'$, then $\psi(m, n) < \psi(m', n')$. If $m = m. \land n < n'$, then we see again $\psi(m, n) < \psi(m', n')$. So $\psi$ is strictly monotonic, therefore it is an injection as we have a total lexicographic ordering of the ordered pairs.\\
\end{proof}
\medskip
\begin{thm}
The set $\mathbb{Q}$ is infinite and countable
\end{thm}
\begin{proof}
If $\mathbb{Q}$ were not infinite, then $\exists t \in \mathbb{Q}, \forall r \in \mathbb{Q}: r \leqslant t$. Every rational can be written as $\frac{p}{q}: p, q \in \mathbb{Z} \land q \neq 0$. The naturals are infinite, implying the integers are infinite, so:
\begin{equation*}
    \exists p, q \in \mathbb{Z}: t = \frac{p}{q} \land q > 0
\end{equation*}
However, since $\mathbb{Z}$ is infinite, $p+1 > p$. So we can construct a rational $s \in \mathbb{Q}$, such that:
\begin{equation*}
    s = \frac{p+1}{q} = \frac{p}{q} + \frac{1}{q} = t + \frac{1}{q}
\end{equation*}
Since $q> 0$, then by field properties $q^{-1} = 1/q > 0$. If $q < 0$, then we simply use $p-1$. So we can show there exists a larger rational, thus $\mathbb{Q}$ is infinite.\\

We can say that the set of positive rationals is countable through the Cantor "Snake" argument by arranging them in a table of naturals minus 0 as rows and as columns. A similar thing can be done with negative integers for the negative rationals. We let $\psi$ be the function that takes in a positive or negative rational and outputs its "place".\\

Each positive and negative rational will appear in the same place by construction (it has the same natural number assigned to it). We formalize it as below:
\begin{equation*}
    \forall r \in \mathbb{Q}^+: \phi(r) = 2\psi(r) = 2n
\end{equation*}
Here $n$ is the place of the positive rational in the Cantor Snake argument. For negative rationals:
\begin{equation*}
    \forall q \in \mathbb{Q}^-: \phi(q) = 2\psi(q) - 1 = 2n-1
\end{equation*}
Finally $\phi(0) = 0$. So all the negative rationals map to the odd naturals, while the positive rationals map to the even naturals. We see that this is an injection as if two rationals have the same value of $\phi$, then they must be both greater/less than 0.\\

Since the set of positive/negative rationals are bijective to the naturals, then the two rationals must be unique. Thus $\text{Ran}(\phi) \subseteq \mathbb{N}$, and because the rationals are infinite, the range is infinite. Thus by Thm 6.1, the range is countable.
\end{proof}

\medskip
\begin{lem}
The set of algebraic reals: $\{x \in \mathbb{R}: (\exists n \in \mathbb{N}, \exists a_0,...,a_n \in \mathbb{Z}: a_n \neq 0 \land a_nx^n +...+a_1x + a_0 = 0)\}$ is countable.
\end{lem}
We first prove that an $n$ degree polynomial has at most $n$ roots:
\begin{proof}
Our base case is when $n= 0$. This polynomial does not have roots trivially.\\

We assume that a polynomial $p(x)$ that is degree $n$ with at most $n$ roots.\\

Then if there is a $q(x) = (x- r)p(x)$ such that $\deg(q) = \deg(p) + 1 = n+1$, if $p(r) \neq 0$, then $\# \ \text{roots(q)} = \# \ \text{roots(p)} \leqslant \deg(p) < \deg(q)$.\\

If $p(r) = 0$, then $\# \ \text{roots(q)} = \# \ \text{roots(p)} + 1 \leqslant \deg(p) + 1 = \deg(q)$. Therefore for any polynomial $\# \ \text{roots(q)} \leqslant \deg(q)$

\vspace{6pt}

We can define each $n$ degree polynomial by the $n+1$-tuple of its coefficients. Thus, we can define the set of polynomials degree $n$:
\begin{equation*}
    P_n \coloneq \mathbb{Z}^n
\end{equation*}
By Thm 6.1, $\mathbb{Z} \subset \mathbb{Q} \implies Z \ \text{countable}$. Thus the cartesian product $\mathbb{Z}^n$ is also countable as $n$ is finite. (If the iterating set were infinite, then we would face an infinite cartesian product).\\

Since each polynomial $p \in P$ has at most $\deg(p)$ roots, then the set of roots for each $P_n$ is also countable. If we define the set of roots for $P_n$ to be $T_n$, then the set of all roots $\bigcup T_n$ is also countable.\\

Thus $\mathbb{R}_A \coloneq \bigcup T_n$, so the set of all algebraic roots is countable.
\end{proof}

\medskip
\begin{thm}
There exists non-algebraic reals.
\end{thm}
Since $\mathbb{R}_A$ is countable and $\mathbb{R}$ is not countable, then $\mathbb{R}_A \lnsim \mathbb{R}$, so we can define the nonalgebraic reals as:
\begin{equation*}
    \mathbb{R}_N \coloneq \mathbb{R} \setminus \mathbb{R}_A
\end{equation*}
We know this set is nonempty by the strict inequality for numerosity.

\pagebreak

\section{Uncountable Sets}
\begin{thm}
The set of sequences $\{0, 1\}^\mathbb{N}$ is uncountable
\end{thm}
We utilize Cantor's Diagonal argument for this theorem.
\begin{proof}
We let $f: \mathbb{N} \xrightarrow{} \{0, 1\}^\mathbb{N}$. We construct a sequence $\{x_n\}_{n \in \mathbb{N}}: x_n = 1 - f(n)_n$.\\

Since every $nth$ term in this new sequence is different from the $nth$ sequence, the sequence does not belong in $\text{Im}(f)$, so $f$ is not surjective, meaning $\{0, 1\}^\mathbb{N}$ is uncountable.
\end{proof}

\medskip
\begin{lem}
The set of functions $\{0, 1\}^\mathbb{R}$ is uncountable
\end{lem}
We once again utilize Cantor's Diagonal argument for this lemma.
\begin{proof}
This time we deal with graphs of functions and not sequences. Each function $f: \mathbb{R} \xrightarrow{} \{0, 1\}$. We also have $\phi: \mathbb{N} \xrightarrow{} \{f\}$.\\

We define $h: \mathbb{R} \xrightarrow{} \{0, 1\}$ as the function where the value of $n$ of $nth$ function is $(n, 1- f_n(n))$.\\

We see that $h$ does not fit in any of the functions, so the set of such functions are uncountable.
\end{proof}

\medskip
\begin{lem}
$\mathbb{N}^\mathbb{N}$ is uncountable.
\end{lem}
\begin{proof}
Assume there to be a countable amount of natural valued sequences. We can construct a new sequence by taking the $nth$ term of the $nth$ sequence and adding 1 to it. Thus we come up with a new sequence not the same as the others, showing it is not countable.
\end{proof}

\medskip
\begin{defn}
Equinumerosity of two sets $A, B$: $A \simeq B \implies \exists f: (f:A \xrightarrow{} B) \land f \ \text{bijection}$
\end{defn}
The Equinumerosity class of a set $[A]$ is the set of all sets equinumerous to $A$.

\medskip
\begin{thm}
For all $A$, $A \notin [\mathcal{P}(A)]$
\end{thm}
\begin{proof}
We want to prove that there exists no bijection $f: A \xrightarrow{} \mathcal{P}(A)$. Since $\forall x \in A, \{x\} \subset A$, then $\forall x \in A: x \in \mathcal{P}(A)$ by definition of powerset. If two such singletons are equal, then by Axiom of Exensionality, their elements must be equal. Thus $f$ is an injection.\\

We prove that $f$ is not a surjection. We know that $A \subseteq A$, so $A \in \mathcal{P}(A)$, but $A \notin A$, so $f$ is not a surjection.\\

If $A \in A$, then we would have a loop of infinite containment, which is not allowed by Axiom of Regularity.
\end{proof}

\medskip
\begin{defn}
We define nonequinumerosity: $A \lesssim B \implies \exists f: (f: A \xrightarrow{} B) \land f \ \text{injection}$
\end{defn}
Cardinality is limited to countable or finite sets, so this inequality allows us to compare numerosity or size for uncountable sets as well.\\

The reason why this is so is because $f(A) \simeq A$ if $f$ is an injection (as it is one to one), and since $f(A) \subseteq B$, then $A \simeq f(A) \lesssim B$.\\

We observe that $A \subseteq B \implies A \lesssim B$. However $A \lesssim B \land B \lesssim A$ does not imply $A = B$. For example:
\begin{equation*}
    \mathbb{Q} \lesssim \mathbb{N} \land \mathbb{N} \lesssim \mathbb{Q} 
\end{equation*}
Yet, $\mathbb{Q} \neq \mathbb{N}$. While we cannot make a conclusion about equality, the Cantor=Bernstein Theorem allows us to say the following:
\begin{thm}
$A \lesssim B \land B \lesssim A \implies A \simeq B$
\end{thm}
\begin{proof}
Proof is left as an excercise to the reader. (I will complete it later, I am the reader)
\end{proof}
However, a powerful result of this theorem is that we only need to show two injections exist between both sets to show they are equinumerous.\\

We build an algebraic structure out of numerosity:
\begin{thm}
Numerosity $\lesssim$ is a partial ordering for a set $U$ of possible sets.
\end{thm}
\begin{enumerate}
    \item Reflexivity: $A \lesssim A$ is trivial as $A = $ so $A \simeq A$
    \item Anti-symmetry: $A \lesssim B \land B \lesssim A \implies A \simeq B$ (given by Cantor Bernstein)
    \item Transitivity: $A \lesssim B \land B \lesssim C \implies A \lesssim C$
\end{enumerate}
We prove transitivity, for which we use the following lemma:
\begin{lem}
Given $f: A \xrightarrow{} B \land g: B \xrightarrow{} C$. If $f, g$ are injections, then $g \circ f$ is an injection.
\end{lem}
\begin{proof}
By the definition of injection, we say that $\forall a, b \in B: g(a) = a(b) \implies a = b$. By preimage we say that:
\begin{equation*}
    \exists x, y \in A: f(x) = a \land f(y) = b
\end{equation*}
So our initial proposition becomes $\forall x, y \in A: g(f(x)) = g(f(y)) \implies f(x) = f(y)$. By injectivity of $f$, we get that $a = b$, so $g \circ f$ is an injection.
\end{proof}

Now the fact that $g \circ f$ is an injection tells us by the definition of numerosity (nonequinumerosity), that $A \lesssim C$, proving transitivity.\\

\medskip
We now prove the following lemma about the Cartesian product (which will contribute as the operation to our algebraic structure):
\begin{lem}
$\forall A, B, C \in U: A \lesssim B \implies A \times C \lesssim B \times C$
\end{lem}
We are given $A \lesssim B$ which means we have an injection $f: A \xrightarrow{} B$. We can define the following function:
\begin{equation*}
    \forall (a, c) \in A \times C: g(a, c) = (f(a), c)
\end{equation*}
We see that this is an injection. Take two ordered pairs $(a, c), (b, d)$, then:
\begin{equation*}
    g(a, c) = g(b, d) \implies (f(a), c) = (f(b), d) \implies f(a) = f(b) \land c = d
\end{equation*}
We derive the equivalence of the components by our construction of ordered pairs with Kuratowski pairs. It also stems from the fact that the two components are from different sets. By the injectivity of $f$:
\begin{equation*}
    f(a) = f(b) \land c = d \implies a = b \land c = d \implies (a, b) = (c, d)
\end{equation*}
Thus $g$ is injective so we see that $A \times C \lesssim B \times C$. The same proof can be carried out to show that $C \times A \lesssim C \times B$. We will see that the inequality does not behave nicely with $\lnsim$.\\

This likely seems similar to the ordered preserving operations on ordered fields, hinting at an underlying algebraic structure.\\

\begin{defn}
A partially ordered semigroup is a group equipped with associativity, identity and order ($\leqslant$), but not with inverses under the group operation.\\
Additionally $\forall a, b, c \in G: a \leqslant b \implies a + g \leqslant b + g \iff g + a \leqslant g + b$
\end{defn}
We see that the cross product $\times$ (our group operation) is associative (we can find a bijection between $A \times (B \times C)$ and $(A \times B) \times C$).\\

By Lemma 7.3, we see that the order is preserved under the operation. We do not have inverse for the cartesian product. For the identity, lets call it $I$:
\begin{equation*}
    A \times I \simeq A
\end{equation*}
This happens when $I$ has cardinality of 1, which means we can have multiple identities. However, the identity of a group must be unique! So we are not dealing with sets but rather equivalence classes of equinumerous sets!\\

The identity $I$ is the class of all sets cardinality 1. Then we get the following property: $[A] \times [B] \simeq [A \times B]$. So when we say that $A \lesssim B$, what we really mean is $[A] \lesssim [B]$, as:
\begin{equation*}
    A' \in [A] \land B' \in [B] \land \exists f: A \xrightarrow{} B \land f \ \text{injection}
\end{equation*}
Then $A' \in [A] \land B' \in [B]$ by Definition 7.1 means we can find bijections $g, h$ between $A', A$ and $B', B$ respectively. Then, we get the following injection:
\begin{equation*}
    g \circ f \circ h: A' \xrightarrow{} B' \implies A' \lesssim B'
\end{equation*}
Thus the choice of representative is arbitrary, but when we say a numerosity inequality is true for certain sets, it is also true for the sets equinumerous to them.

\medskip
\begin{defn}
The equinumerosity class $[\mathbb{R}]$ is called the continuum.
\end{defn}
We can show that $\mathcal{P}(\mathbb{N}) \in [\mathbb{R}]$.
\begin{proof}
$\mathcal{P}(\mathbb{N}) \simeq \{0, 1\}^\mathbb{N}$ via the following bijection:
\begin{equation*}
    \forall A \in \mathcal{P}(\mathbb{N}): f_n(A) \coloneq \begin{cases}
        0: \ \ n \notin A\\
        1: \ \ n \in A
    \end{cases}
\end{equation*}
If two sequences are equal, then we know that their preimages must be the same, so $f$ is injective. We also see $f$ is surjective, since every sequence is the size of $\mathbb{N}$, it obtains an equivalent expression as a subset of $\mathbb{N}$.\\

Before we derived an injection from $\{0, 1\}^\mathbb{N} \xrightarrow{} [0, 1]$, thus we can derive an injection from $\mathcal{P}(\mathbb{N}) \xrightarrow{} [0, 1]$. Since $[0, 1] \subset \mathbb{R}$, the injection follows, so $\mathcal{P}(\mathbb{N}) \lesssim \mathbb{R}$.\\

Now $\mathbb{R}$ is the set of Dedekind cuts to $\mathbb{R} \lesssim \mathcal{P}(\mathbb{Q})$.\\

Since $\mathbb{Q} \simeq \mathbb{N}$ then $\mathcal{P}(\mathbb{Q}) = \mathcal{P}(\mathbb{N})$ (why? Technically by $2^n$ rule but not proven). Thus $\mathbb{R} \lesssim \mathcal{P}(\mathbb{N})$, so:
\begin{equation*}
    \mathcal{P}(\mathbb{N}) \simeq \{0, 1\}^\mathbb{N} \simeq [0, 1] \simeq \mathbb{R}
\end{equation*}
\end{proof}

\medskip
\begin{lem}
$\mathbb{R} \times \mathbb{R} \simeq \mathbb{R}$ and $\forall n \in \mathbb{N}: \mathbb{R}^n \simeq \mathbb{R}$
\end{lem}
We first prove that $\mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \mathcal{P}(\mathbb{N})$. We see trivially that $\mathcal{P}(\mathbb{N}) \lesssim \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N})$.
\begin{equation*}
    \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \{0, 1\}^\mathbb{N} \times \{0, 1\}^\mathbb{N}
\end{equation*}
We can construct an injection $f: \{0, 1\}^\mathbb{N} \times \{0. 1\}^\mathbb{N} \xrightarrow{} \{0, 1\}^\mathbb{N}$. If we take a pair of sequences, simply concatenating them provides a new, unique sequence. Therefore:
\begin{equation*}
    \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \{0, 1\}^\mathbb{N} \times \{0, 1\}^\mathbb{N} \lesssim \{0, 1\}^\mathbb{N} \simeq \mathcal{P}(\mathbb{N})
\end{equation*}
Thus $\mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \mathcal{P}(\mathbb{N})$.

\vspace{6pt}

\begin{proof}
We note that $\mathbb{R} \lesssim \mathbb{R} \times \mathbb{R}$ trivially, as the real numbers are embedded in a 2D Euclidean space.\\
\begin{equation*}
    \mathbb{R} \times \mathbb{R} \lesssim \mathcal{P}(\mathbb{Q}) \times \mathcal{P}(\mathbb{Q}) \simeq \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \mathcal{P}(\mathbb{N}) \simeq \mathbb{R}
\end{equation*}
Thus $\mathbb{R} \times \mathbb{R} \lesssim \mathbb{R}$, so $\mathbb{R} \times \mathbb{R} \simeq \mathbb{R}$.
\end{proof}

\vspace{6pt}

\begin{proof}
We now prove that $\forall n \geqslant 1: \mathbb{R}^n \simeq \mathbb{R}$. The inequality $\mathbb{R} \lesssim \mathbb{R}^n$ is trivial. We prove the other direction:
\begin{equation*}
    \mathbb{R}^n \lesssim \mathcal{P}(\mathbb{Q})^n \simeq \mathcal{P}(\mathbb{N})^n
\end{equation*}
Since $n \in \mathbb{N}$, if $n$ is even, then we create $n/2$ number of pairs of $\mathcal{P}(\mathbb{N})$. Using the property initially proved, we get:
\begin{equation*}
    \mathcal{P}(\mathbb{N})^n \simeq \mathcal{P}(\mathbb{N})^{n/2} \simeq \mathcal{P}(\mathbb{N})^{n/4} \simeq...
\end{equation*}
We repeat this process till we get $\mathcal{P}(\mathbb{N})$. If $n$ is odd, then we apply this process to $n-1$. We then get:
\begin{equation*}
    \mathcal{P}(\mathbb{N})^n \simeq \mathcal{P}(\mathbb{N})^{n-1} \times \mathcal{P}(\mathbb{N}) \simeq \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \mathcal{P}(\mathbb{N})
\end{equation*}
Therefore we complete our previouos inequality:
\begin{equation*}
    \mathbb{R}^n \lesssim \mathcal{P}(\mathbb{Q})^n \simeq \mathcal{P}(\mathbb{N})^n \simeq \mathcal{P}(\mathbb{N}) \simeq \mathbb{R}
\end{equation*}
Now tha we have both inequalities, we have proven that $\mathbb{R}^n \simeq \mathbb{R}$.
\end{proof}

\vspace{6pt}

\begin{lem}
$\mathbb{R}^\mathbb{N} \simeq \mathbb{R}$
\end{lem}
We know that $\mathbb{R} \simeq \{0, 1\}^\mathbb{N}$, so by Lemma 6.3 and 7.2:
\begin{equation*}
    \mathbb{R}^\mathbb{N} \simeq \{0, 1\}^{\mathbb{N} \times \mathbb{N}} \simeq \{0, 1\}^{\mathbb{N}} \simeq \mathcal{P}(\mathbb{N}) \simeq \mathbb{R}
\end{equation*}

\vspace{6pt}

\begin{lem}
$\mathbb{R} \lnsim \mathbb{R}^\mathbb{R}$
\end{lem}
We first prove that $\mathbb{N} \times \mathbb{R} \simeq \mathbb{R}$. The inequality $\mathbb{R} \lesssim \mathbb{N} \times \mathbb{R}$ is trivial. We prove the other direction by Lemma 7.2:
\begin{equation*}
    \mathbb{N} \times \mathbb{R} \simeq \mathbb{N} \times \mathcal{P}(\mathbb{N}) \lesssim \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \simeq \mathcal{P}(\mathbb{N}) \simeq \mathbb{R}
\end{equation*}
\begin{proof}
We can now track the following inequality:
\begin{equation*}
    \mathbb{R} \lnsim \mathcal{P}(\mathbb{R}) \simeq \{0, 1\}^\mathbb{R} \simeq \{0, 1\}^{\mathbb{N} \times \mathbb{R}} \simeq \mathcal{P}(\mathbb{N})^\mathbb{R} \simeq \mathbb{R}^\mathbb{R}
\end{equation*}
\end{proof}
\pagebreak

\section{Metric Spaces}
We earlier defined a sequence as the graph of a function $f: \mathbb{N} \xrightarrow{} A$. While this is the underlying machinery, we consider a sequence as a set of elements indexed by the naturals, denoted as $\{x_n\}_{n \in \mathbb{N}}$.\\

First we deal with "real-valued sequences", which means that all $x_n \in \mathbb{R}$. We define two important notions, in the context of the reals:
\begin{defn}
A Cauchy sequence: $\forall k \in \mathbb{N}, \exists n_0 \in \mathbb{N}, \forall m, n \geqslant n_0: |x_m - x_n| < \frac{1}{k+1}$
\end{defn}
The reason we set the bound as $\frac{1}{k+1}$ is that the bound will always exist in the reals and as we incresae $k$, the bound will get smaller, which is the behavior we want to model. We could set the bound to $\frac{1}{r+1}$, where $r$ is a real, but the natural numbers allos us to create a sequence of bounds unlike the reals which are uncountable.\\

\medskip
\begin{defn}
A convergent sequence: $\exists L \in \mathbb{R}, \forall k \in \mathbb{N}, \exists n_0 \in \mathbb{N}, \forall n \geqslant n_0: |x_n - L| < \frac{1}{k+1}$
\end{defn}
This is different from a Cauchy sequence as we are now fixing a set "limit point". As we decrease the bound, we can always find some element in the sequence arbitrarily close. So for a very large $k$ we see that $x_n$ tends to $L$.\\

\textbf{Example}: We show the following is Cauchy and convergent:
\begin{equation*}
    a_0 = 1 \land a_{n+1} = 3 - \frac{1}{a_n}
\end{equation*}
We show that $\forall n \in \mathbb{N}: 1 \leqslant a_{n+1} \leqslant 3$ and $a_n < a_{n+1}$:
\begin{equation*}
    1 \leqslant a_n \leqslant 3 \implies -1 \leqslant -\frac{1}{a_n} \leqslant -\frac{1}{3}
\end{equation*}
\begin{equation*}
    a_{n+1} \leqslant 3 - \frac{1}{a_n} \leqslant 3 - \frac{1}{3} < 3
\end{equation*}
Also $a_n \geqslant 1 \implies a_{n+1} > a_n \geqslant 1$, so $1 \leqslant a_{n+1} \leqslant 3 \land a_n < a_{n+1}$. Since each consecutive term is greater, we call this monotone (monotonically increasing).\\

We now prove the sequence is Cauchy:
\begin{proof}
We need to show we can find some $n_0$ such that $\forall k \in \mathbb{N}$ the proposition holds true. We apply the triangle inequality:
\begin{equation*}
    |a_{n+1} - a_n| \leqslant \left|3 - \frac{1}{a_n} - (3 - \frac{1}{a_{n-1}})\right| = \left|\frac{1}{a_{n-1}} - \frac{1}{a_n}\right|
\end{equation*}
\begin{equation*}
    = \left|\frac{a_n - a_{n-1}}{a_{n}a_{n+1}}\right| \leqslant \frac{1}{2}|a_n - a_{n-1}| \leqslant \frac{1}{2^n}|a_1 - a_0| = \frac{1}{2^n} < \frac{1}{k+1}
\end{equation*}
We see that $2^{n_0} > k+1$. We know that $\forall n \in \mathbb{N}: n+1 \leqslant 2^n$, so if we set $n_0 \coloneq k+1$, then $k+1 < 2^{k+1}$ and $\frac{1}{2^{k+1}} = \frac{1}{k+1}$.
\end{proof}
\pagebreak

\subsection{Metrics}
We now generalize the notion of distance to the concept of a "metric":
\begin{defn}
A metric space $(X, \varrho)$ is a set $X$ equipped with a metric $\varrho: X \times X \xrightarrow{} \mathbb{R}^+$:
\begin{enumerate}
    \item $\forall x, y \in X: \varrho(x, y) \geqslant 0 \land \varrho(x, x) = 0$ (Positivity)
    \item $\forall x, y \in X: \varrho(x, y) = \varrho(y, x)$ (Symmetry)
    \item $\forall x, y, z \in X: \varrho(x, z) \leqslant \varrho(x, y) + \varrho(y, z)$ (Triangle Inequality)
\end{enumerate}
\end{defn}
We will now look at various metrics. The typical Euclidean metric (distance between two points in $\mathbb{R}^n$) is defined as:
\begin{equation*}
    (\sum_{i=1}^{n}|x_i - y_i|^2)^{\frac{1}{2}}
\end{equation*}
We can in fact generalize this notion to create $p$-metrics for $\mathcal{l}^p$ spaces (which are spaces of $X$ valued sequences with a $p$-norm. Think, vector spaces with a fixed basis such that the elements can be represented as finite sequences, or infinite with Axiom of Choice):
\begin{equation*}
    \text{p-metric} \coloneq (\sum_{i=1}^{n}|x_i - y_i|^p)^{\frac{1}{p}}
\end{equation*}
The $\infty$-metric is then defined as the following and is referred to as the "box metric":
\begin{equation*}
    \max_{i = 1, 2, ..., n}|x_i - y_i|
\end{equation*}
We address the "shapes" of these metrics. This construct stems from the idea of a norm, attached to a vector space (which creates a normed vector space):
\begin{defn}
A norm $||\cdot||$ is a metric that gives the magnitude of a vector from the "origin" or 0 element. Let $V$ be a vector space:
\begin{enumerate}
    \item $\forall v \in V: ||v|| \geqslant 0$ (Positivity)
    \item $\forall v \in V \land c \in F: ||cv|| = |c|||v||$ (Scalar Multiplication)
    \item $\forall v, w \in V: ||v + w|| \leqslant ||v|| + ||w||$ (Triangle Inequality)
\end{enumerate}
\end{defn}
The norm pretty closely mirrors the properties of a metric space (that is because it is, or it induces one). We can derive the remaining properties of the metric space (which is symmetry) by the following:
\begin{equation*}
    \forall v, w \in V: ||v - w|| = ||-(w-v)|| = |-1||w-v|| = ||w-v||
\end{equation*}
This completes all the properties needed for the vector space to be a metric space. We can now draw out the $p$-norm unit circles (which are the set of points/vectors a distance 1 away from the origin):
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth, height = 6cm]{pnorms.png}
    \caption{Unit circles of $p$-norms}
    \label{fig:myimage}
\end{figure}
We will return to verifying the $p$-metrics to be valid metrics via the norms. The $p$-norm now becomes the distance between the origin and the vector:
\begin{equation*}
    \forall v \in V: ||v||_p = (\sum_{k=1}^{n}(|v_k|^p))^{\frac{1}{p}}
\end{equation*}

With the introduction of metrics, we can now generalize the notion of Cauchy and convergent sequences for an $X$ valued sequence $\{x_n\}_{n\in \mathbb{N}}$ with a metric $(X, \varrho)$:
\begin{defn}
A Cauchy sequence: $\forall \epsilon > 0, \exists n_0 \geqslant 0, \forall m, n \geqslant n_0: \varrho(x_m, x_n) < \epsilon$\\
A convergent sequence: $\exists L \in X, \forall \epsilon > 0, \exists n_0 \geqslant 0, \forall n \geqslant n_0: \varrho(x_n, L) < \epsilon$
\end{defn}
The definition is largely similar, with the difference being an arbitrary upper bound $\epsilon$ as it is not specified what the set $X$ is and what the metric $\varrho$ is.\\

\begin{lem}
A convergent sequence is Cauchy.
\end{lem}
\begin{proof}
Given $(X, \varrho)$ and a sequence $\{x_n\}_{n\in \mathbb{N}}$, we let the limit point be $L$. Then:
\begin{equation*}
    \forall \epsilon > 0, \exists n_0 \geqslant 0, \forall n \geqslant n_0: \varrho(x_n, L) < \epsilon/2
\end{equation*}
We choose an $m \geqslant n_0$ such that $\varrho(x_m, L) < \epsilon/2$. Then by the triangle inequality:
\begin{equation*}
    \varrho(x_n, x_m) \leqslant \varrho(x_n, L) + \varrho(L, x_m) = \varrho(x_n, L) + \varrho(x_m, L) = \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{equation*}
Thus $\forall \epsilon > 0$, we have shown there exists an $n_0$, which is the same as our initial one in the limit defintion, such that $\forall m, n \geqslant n_0: \varrho(x_m, x_n) < \epsilon$.
\end{proof}

\medskip
\begin{lem}
For $(X, \varrho)$, an $X$ valued, convergent sequence $\{x_n\}_{n_k \in \mathbb{N}}$ has at most one limit $L$. 
\end{lem}
\begin{proof}
We assume there to be two limits $L$ and $L'$ such that $x_n \xrightarrow{} L \land x_n \xrightarrow{} L'$. Since $L \neq L'$, then we can find an $\epsilon$ such that $\varrho(L, L') > \epsilon$. Then we define an $n_0$, such that:
\begin{equation}
    \forall n \geqslant n_0: \varrho(x_n, L) < \frac{\epsilon}{2} \land \varrho(x_n, L') < \frac{\epsilon}{2}
\end{equation}
By positivity we know that $\varrho(x, x) = 0$, so by the triangle inequality:
\begin{equation*}
    \varrho(L, L') \leqslant \varrho(x_n, L) + \varrho(x_n, L')  = \epsilon < \varrho(L, L')
\end{equation*}
We get that $\varrho(L, L') < \varrho(L, L')$ which is a contradiction by metric properties, so $L = L'$.
\end{proof}

\medskip
\begin{defn}
A subsequence is defined by $\{x_{n_k}\}_{n_k \in \mathbb{N}}$ where $\{n_k\}_{k \in \mathbb{N}}$ is a strictly increasing/monotonically increasing sequence
\end{defn}
Typically we can define the $n_k$ sequence by some rule, such as even naturals or odd naturals.

\medskip
\begin{lem}
For a Cauchy sequence $\{x_n\}_{n \in \mathbb{N}}$, if there is a convergent subsequence such that $x_{n_k} \xrightarrow{} z \implies x_n \xrightarrow{} z$ where $z$ is a limit point
\end{lem}
\begin{proof}
The sequence being Cauchy means $\forall \epsilon > 0, \exists n_0 \geqslant 0, \forall m, n \geqslant n_0: \varrho(x_m, x_n) < \epsilon/2$.\\

For the convergent subsequence, we get that $\exists z \in X, \forall \epsilon > 0, \exists n_1 \geqslant 0, \forall n \geqslant n_1: \varrho(x_n, z) < \epsilon/2$.\\

Then we choose an $m \geqslant n_1 \land m \geqslant n_0 \land n \geqslant n_0 \land n \geqslant n_1$. By triangle inequality:
\begin{equation*}
    \varrho(x_m, z) \leqslant \varrho(x_m, x_n) + \varrho(x_n, z) = \epsilon
\end{equation*}
Thus the sequence for any $m \geqslant n_1 \land m \geqslant n_0$ converges to $z$. We can simply set the $n_2 = \max\{n_0, n_1\}$ such that:
\begin{equation*}
    \exists z \in X, \forall \epsilon > 0, \exists n_2 \geqslant 0, \forall n \geqslant n_2: \varrho(x_n, z) < \epsilon
\end{equation*}
\end{proof}

\medskip
\textbf{Intuition Note:} We use $\epsilon > 0$, as if $\epsilon = 0$, then that would mean the two values being compared are equal. However if you take the $nth$ term for a very vary large $n$ in a sequence, not necessarily you will reach the limit, but you may be ever so close, which is why there will be some small difference $\epsilon$. However, if we say it equals $\epsilon$ and then choose the next term of the sequence, then the difference will not necessarily be equal to. However, if it is converging, then it will likely be less, so we say $< \epsilon$. If we choose the $\epsilon$ then we can guarantee a closer term to the limit always.\\

What if the sequence does actually reach the limit? We view the discrete metric for a point $y$:
\begin{equation*}
    \varrho(x, y) \coloneq \begin{cases}
        0: \ x = y\\
        1: \ x \neq y
    \end{cases}
\end{equation*}
\begin{cor}
If a sequence in $(X, \varrho)$ is Cauchy, then it is convergent and eventually constant.
\end{cor}
\begin{proof}
If it is Cauchy, then $\forall \epsilon > 0, \exists n_0 \geqslant 0, \forall m, n \geqslant n_0: \varrho(x_m, x_n) < \epsilon$. For any epsilon (eg $\epsilon = 1$), the metric being less than epsilon implies that $x_m = x_n$. So we can define a limit $L$ such that $x_m = x_n = L$.\\

We can see that the sequence does meet and stagnates at the limit under this metric. 
\end{proof}

\medskip
\begin{lem}
Urysohn's Subsequence Principle says the following are equivalent for a sequence $\{x_n\}_{n \in \mathbb{N}}$ and a limit point $z$:
\begin{enumerate}
    \item $x_n \xrightarrow{} z$
    \item Every subsequence $\{x_{n_k}\}_{n_k \in \mathbb{N}}$ contains a convergent subsubsequence such that $x_{{n_k}_j} \xrightarrow{} z$
\end{enumerate}
\end{lem}
\begin{proof} We prove equivalence of the statements by showing an if and only if relation:\\

$\implies$: By lemma 8.1, we get that the sequence is Cauchy. We apply Lemma 8.3 twice to see that there exists such a subsubsequence converging to $z$.\\

$\impliedby$: We proceed by contrapositive. Assume $\{x_n\}_{n \in \mathbb{N}}$ does not converge at all, then $\forall \epsilon > 0, \exists n_0 \geqslant 0, \forall n \geqslant n_0: \varrho(x_n, z) \geqslant \epsilon$. If we then set:
\begin{equation*}
    n_0 = 1 \land n_k \coloneq \inf\{m > n_k \land \varrho(x_m, z) > \epsilon\}
\end{equation*}
Essentially each $n_k$ of the subsequence is the smallest value of $m$ so that the sequence does not converge, then the subsequence does not converge. So by the contrapositive of lemma 8.3, the subsubsequence does not converge.
\end{proof}

\medskip
\begin{note}
\textbf{Tips for Convergence Proofs}: If the $n$ term is in the exponent, eg $1/n$ try to use archimedian to bring it to the numerator. For example:
\begin{equation*}
    \forall \epsilon > 0, \exists n \in \mathbb{N} \setminus \{0\}: b^{1/n} - 1 < \epsilon
\end{equation*}
\begin{equation*}
    b^{1/n} - 1 \leqslant b - 1 < \epsilon \implies \frac{\epsilon}{b-1} > 0 \implies \exists n: n\cdot \frac{\epsilon}{b-1} > 1
\end{equation*}
\begin{equation*}
    \implies n > \frac{b-1}{\epsilon} \implies n \coloneq \frac{b-1}{\epsilon} + 1
\end{equation*}
Also if we are trying to show less than or equal to inequalities, we want to find a \textbf{LARGER} numerator OR a \textbf{SMALLER} denominator.\\

To determine monotonicity of a sequence (or subsequence) with a recursive definition:
\begin{enumerate}
    \item Check the base case e.g. $a_0 < a_1$
    \item Assume $P_n$ such that $a_{n-1} < a_n$
    \item Write the (recursive) sequence as a function and show it is increasing or decreasing
    \item Apply it to $P_n$ to show $a_n < a_{n+1}$ is true, thus concluding for all $n$
    \item This can help us eliminate certain candidates for limits
\end{enumerate}
\end{note}

\medskip
\textbf{Midterm 2 Problem 5}: Prove that $d(x, y) \coloneq \inf\{\varrho(x, w) + \varrho(w, y): w \in X\}$ is a metric.
\begin{proof}
We will skip over symmetry and positivity (those are easy to prove). For the triangle inequality, we need to prove the following:
\begin{equation*}
    d(x, y) \leqslant d(x, z) + d(z, y) \implies \inf\{\varrho(x, w) + \varrho(w, y)\} \leqslant \inf\{\varrho(x, w) + \varrho(w, z)\} + \inf\{\varrho(z, w) + \varrho(w, y)\}
\end{equation*}
We know that by infimum properties that $\inf(A) + \inf(B) = \inf(A + B)$. So:
\begin{equation*}
    \inf\{\varrho(x, w) + \varrho(w, z)\} + \inf\{\varrho(z, w) + \varrho(w, y)\} = \inf\{\varrho(x, w) + \varrho(w, y) + 2\varrho(w, z)\}
\end{equation*}
We need to prove the infimum of the left set is less than the infimum of the right hand set. We see that by positivity $\varrho(w, z) \geqslant 0$:
\begin{equation*}
    \forall w \in X: \varrho(x, w) + \varrho(w, y) \leqslant \varrho(x, w) + \varrho(w, y) + 2\varrho(w, z)
\end{equation*}
The left hand side of the inequality is therefore always a bound for the elements of the right hand set so:
\begin{equation*}
    \forall w \in X: \inf\{\varrho(x, w) + \varrho(w, y)\} \leqslant \varrho(x, w) + \varrho(w, y) \leqslant \inf\{\varrho(x, w) + \varrho(w, y) + 2\varrho(w, z)\}
\end{equation*}
Thereby proving the triangle inequality for this metric.
\end{proof}
\pagebreak

\section{Topology}
\subsection{Open and Closed sets}
We now deal with the notion of a set being "open" and "closed". This is the same as saying does the set contain its boundary points or not (however we have not defined boundary points).\\

\begin{defn}
An open ball around point $x$, radius $r$ in $(X, \varrho)$ is $B(x, r) \coloneq \{y \in X: \varrho(x, y) < r\}$
\end{defn}
We talked about convergent and Cauchy sequences using the idea of a metric (distance between points) given to a set $X$.\\

With the Euclidean metric $p_2$, the ball in 1D is an open interval. In 2D it is a circle, and in 3D a sphere, and so on so forth. We saw that in 2D for the $||\cdot||_\infty$, we get the shape of a square.\\

This is equivalent to saying the open ball is the cartesian product of two open intervals, so in $(\mathbb{R}^n, p_\infty)$:
\begin{equation*}
    B_\infty(x, r) = \bigtimes_{i = 1}^{n}(x_i - r, x_i + r)
\end{equation*}
We can use this to see if a set is open:
\begin{defn}
An open set is one for which we can find an empty ball around each point that is a subset of the set
\end{defn}
\textbf{Note the IFF}: A set is closed if and only if its complement is open.\\

\begin{cor}
A set $A$ is open if and only if its complement is closed.
\end{cor}
$\impliedby$: $A^c$ being closed means its complement is open by Definition 9.2. $(A^c)^c = A$, so $A$ is open.\\

$\implies$:

\begin{lem}
The singleton $\{x\}$ is a closed set.
\end{lem}
Assume $x \in (X, \varrho)$, then for any $r \in \mathbb{R}$, we can find some point $y \in X$ such that $\varrho(x, y) = r$. If we then set the open ball around $x$ to be $B(y, r)$, then it will not contain $x$ by definition of open ball.\\

We can construct such open balls for all $y \in X \setminus \{x\}$ so by definition 9.2, $X \setminus \{x\}$ is open, thus $\{x\}$ is closed.\\

\begin{lem}
Every "open" ball is open
\end{lem}
We take any point in the open ball $y \in B(x, r)$. By definition $\varrho(x, y) < r$, we can then define an open ball around $y$:
\begin{equation*}
    B(y, r') \coloneq \{z \in X: \varrho(y, z) < r'\}
\end{equation*}
Then $\varrho(y, z) < r' \land r' + \varrho(x, y) < r$. If we set $\varrho(y, z) < r - \varrho(x, y)$. By triangle inequality:
\begin{equation*}
    \varrho(x, z) \leqslant \varrho(x, y) + \varrho(y, z) < \varrho(x, y) + r - \varrho(x, y) = r
\end{equation*}
Since $\varrho(x, z) < r$, then by definition $z \in B(x, r)$, so we can always find some open ball around each point in the open ball such that it belongs to $B(x, r)$.\\

Remember that our lemma is equivalent to the statement that we can find an open ball around each point in parent ball such that it is a subset of the parent ball. So we need to find a radius of the smaller ball around each point that allows us to say that.
\begin{lem}
If $\varrho$ is a discrete metric on $X$, then every subset of $X$ is open and closed.
\end{lem}
If we consider the open ball $B(x, r)$ with $0 < r < 1$, then the only point contained within this ball is the singleton $\{x\}$. By lemma 9.1, this is closed, however, since its an open ball it is open as well.\\

For any subset $A \subseteq X$, if we take the discrete points of $A$ as singletons, then each point in $A$ has an open ball that is a subset of $A$. Thus by Definition 9.2, A is open.\\

We can depend on the same construction for $X \setminus A$ which becomes open, and since $A^c = X \setminus A$, then $A$ is a closed set by Definition 9.2.\\

\begin{lem}
A closed ball $\bar{B}(x, r) \coloneq \{y \in X: \varrho(y, x) \leqslant r\}$ is closed
\end{lem}
The complement of this set is $A \coloneq \{y \in X: \varrho(y, x) > r\}$. We aim to show that this set is open:\\
\begin{equation*}
    \forall y \in A: B(y, r') \coloneq \{z \in X: \varrho(y, z) < r'\}
\end{equation*}
We define open balls in $A = \bar{B}^c$ and just like Lemma 9.2 find a suitable radius r'. By the triangle inequality:
\begin{equation*}
    \varrho(x, y) \leqslant \varrho(x, z) + \varrho(y, z) \implies \varrho(x, y) - \varrho(y, z) \leqslant \varrho(x, z)
\end{equation*}
Using the inequality $\varrho(y, z) < r'$, we see that:
\begin{equation*}
    r - r' < \varrho(x, y) - r' < \varrho(x, z)
\end{equation*}
So if we set $0 < r' < \varrho(x, y) - r$, we see that $\varrho(x, z) > r$ which means $z \in A \lor B(y, r') \subseteq A$. We also know that $\varrho(x, y) - r > 0$ by definition of $A$. Since $A$ is open, then $\bar{B} = A^c$ is closed.
\end{document}
